import sys
import os

# Add root directory for imports
sys.path.append(os.path.dirname(os.path.abspath(os.path.dirname(__file__))))

import argparse
import pandas as pd
import ast
import json
from tqdm import tqdm
from time import time
from statistics import mean

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from config import Config

# Import Pipeline
# Add 02_advanced_rag to path
sys.path.append(
    os.path.join(
        os.path.dirname(os.path.dirname(os.path.abspath(__file__))), "02_advanced_rag"
    )
)
try:
    from pipeline import HighContextRAGPipeline
except ImportError:
    # Fallback if run from different dir
    from .pipeline import HighContextRAGPipeline


def convert_matches_to_score(match_dict):
    count = sum(match_dict.values())
    if count >= 4:
        return 1.0
    elif count >= 2:
        return 0.6
    elif count == 1:
        return 0.2
    else:
        return 0.0


def run_local_evaluation(csv_path, output_path, limit=None, run_count=1):
    print("ğŸš€ Starting Local Hybrid Evaluation...")

    # 1. Init Pipeline
    try:
        pipeline = HighContextRAGPipeline()
        print("âœ… HighContextRAGPipeline Initialized (Hybrid Target)")
    except Exception as e:
        print(f"âŒ Failed to init pipeline: {e}")
        return

    # 2. Init Judge LLM (GPT-4o)
    api_key = os.getenv("OPENAI_API_KEY") or Config.OPENAI_API_KEY
    if not api_key:
        print("âš ï¸ Warning: OPENAI_API_KEY not found. Scoring will be skipped.")
        judge_llm = None
    else:
        judge_llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=api_key)
        print("âœ… Judge LLM (GPT-4o-mini) Ready")

    # 3. Load Data
    try:
        df = pd.read_csv(csv_path)
        print(f"ğŸ“„ Loaded {len(df)} queries from {csv_path}")
    except Exception as e:
        print(f"âŒ Failed to load CSV: {e}")
        return

    if limit:
        df = df.head(limit)

    # 4. Evaluation Loop (Multi-Run)
    all_runs_results = []

    # Scoring Prompt
    system_prompt = """
ë‹¹ì‹ ì€ ê°ì‚¬ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸ ë¬¸ì„œ ìœ ì‚¬ë„ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì•„ë˜ ë‹¤ì„¯ ê°€ì§€ ê¸°ì¤€ì„ ì‚¬ìš©í•´ ì§ˆë¬¸(question)ê³¼ ë¬¸ì„œ(document)ê°€ ìœ ì‚¬í•œì§€ íŒë‹¨í•˜ì‹­ì‹œì˜¤.
íŒì •ì€ ë§¤ìš° ì—„ê²©í•˜ê²Œ ìˆ˜í–‰í•˜ë©° TrueëŠ” ê¸°ì¤€ì„ ëª…í™•íˆ ì¶©ì¡±í•  ë•Œë§Œ ì„ íƒí•˜ì‹­ì‹œì˜¤.

[í‰ê°€ ê¸°ì¤€]

1. ì£¼ì œ ì¼ì¹˜(Topic Match)
   ì§ˆë¬¸ê³¼ ë¬¸ì„œê°€ ë‹¤ë£¨ëŠ” ê°ì‚¬ ë¶„ì•¼ê°€ ì„¸ë¶€ ë¶„ì•¼ ìˆ˜ì¤€ì—ì„œ ì™„ì „íˆ ë™ì¼í•  ë•Œë§Œ Trueë¡œ íŒë‹¨í•©ë‹ˆë‹¤.
   ì¶œì¥ë¹„ ê³„ì•½ ìˆ˜ì˜ê³„ì•½ ë“± ë™ì¼í•œ ë¶„ì•¼ì—¬ì•¼ í•˜ë©° í° ë²”ì£¼ê°€ ë¹„ìŠ·í•˜ê±°ë‚˜ ë‚´ë¶€í†µì œ ê°™ì€ ì¼ë°˜ ë‹¨ì–´ë§Œ ê²¹ì¹˜ëŠ” ê²½ìš°ëŠ” Falseì…ë‹ˆë‹¤.

2. ì„¸ë¶€ìŸì  ì¼ì¹˜(Sub Issue Match)
   ì§ˆë¬¸ì´ ìš”êµ¬í•˜ëŠ” í•µì‹¬ ìŸì ì´ ë¬¸ì„œì—ì„œ ë‹¤ë£¨ëŠ” êµ¬ì²´ì  ë¬¸ì œì™€ ì§ì ‘ì ìœ¼ë¡œ ì¼ì¹˜í•  ë•Œë§Œ Trueì…ë‹ˆë‹¤.
   ì˜ˆë¥¼ ë“¤ì–´ ì§ˆë¬¸ì´ ì¶œì¥ë¹„ ì¦ë¹™ ëˆ„ë½ ë¬¸ì œë¥¼ ë¬»ëŠ” ê²½ìš° ë¬¸ì„œì—ë„ ì¦ë¹™ ëˆ„ë½ ë¬¸ì œë‚˜ ê´€ë ¨ ë¶€ì ì • ì§€ê¸‰ ìŸì ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
   ê°™ì€ ë¶„ì•¼ë¼ë„ ë¬¸ì œ í¬ì¸íŠ¸ê°€ ë‹¤ë¥´ë©´ Falseì…ë‹ˆë‹¤.

3. ì‚¬ê±´ ë©”ì»¤ë‹ˆì¦˜ ìœ ì‚¬(Case Mechanism Match)
   ë¬¸ì œ ë°œìƒ ê³¼ì •ê³¼ ì‚¬ê±´ ì „ê°œ ë°©ì‹ì´ ì§ˆë¬¸ê³¼ ë¬¸ì„œì—ì„œ ë™ì¼í•œ ê²½ìš°ì—ë§Œ Trueì…ë‹ˆë‹¤.
   ì ˆì°¨ ë¯¸ì¤€ìˆ˜ë¡œ ì¸í•œ ë¶€ë‹¹ ì§€ê¸‰ê³¼ ê°™ì€ ë‹¨ê³„ì  êµ¬ì¡°ê°€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
   ê²°ê³¼ë§Œ ë¹„ìŠ·í•˜ê±°ë‚˜ ì›ë¦¬ êµ¬ì¡°ê°€ ë‹¤ë¥´ë©´ Falseì…ë‹ˆë‹¤.

4. ìœ„ë°˜í–‰ìœ„ íŒ¨í„´ ìœ ì‚¬(Violation Pattern Match)
   ë¶€ì •í–‰ìœ„ì˜ ìœ í˜•ì´ ì§ˆë¬¸ê³¼ ë¬¸ì„œ ì–‘ìª½ì—ì„œ ë™ì¼í•  ë•Œ Trueì…ë‹ˆë‹¤.
   í—ˆìœ„ ì²­êµ¬ ë¶€ë‹¹ ì§€ê¸‰ ê·œì • ë¯¸ì¤€ìˆ˜ ë“± ìœ„ë°˜ íŒ¨í„´ì´ ì™„ì „íˆ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
   ìœ í˜•ì´ ë‹¤ë¥´ë©´ Falseì…ë‹ˆë‹¤.

5. ì›ì¸ êµ¬ì¡° ìœ ì‚¬(Cause Pattern Match)
   ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸ì´ ì§ˆë¬¸ê³¼ ë¬¸ì„œì—ì„œ ë™ì¼í•  ë•Œë§Œ Trueì…ë‹ˆë‹¤.
   ë‚´ë¶€í†µì œ ë¯¸í¡ ê´€ë¦¬ ê°ë… ì†Œí™€ ê·œì • ë¯¸ë¹„ ë“± ì›ì¸ ì²´ê³„ê°€ ê°™ì•„ì•¼ í•©ë‹ˆë‹¤.
   ê°œì¸ ì¼íƒˆì´ë‚˜ ê³ ì˜ì  ë¹„ìœ„ì²˜ëŸ¼ ë‹¤ë¥¸ êµ¬ì¡°ë¼ë©´ Falseì…ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹]
ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.

{{
  "topic_match": true/false,
  "subtopic_match": true/false,
  "case_structure_match": true/false,
  "violation_pattern_match": true/false,
  "cause_pattern_match": true/false
}}

ì¶”ê°€ ì„¤ëª… ì´ìœ  í•´ì„ ë¬¸ì¥ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
"""
    eval_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "[Question]\n{question}\n\n[Document]\n{document}"),
        ]
    )

    for run_idx in range(run_count):
        print(f"\nâ–¶ï¸ Running Retrieval Eval Batch {run_idx + 1}/{run_count}...")
        results = []

        for i, row in tqdm(
            df.iterrows(), total=len(df), desc=f"Evaluating Run {run_idx + 1}"
        ):
            query = row.get("question") or row.get("query")
            if not query:
                continue

            # A. Retrieval (Hybrid)
            start_t = time()
            # Note: HighContextRAGPipeline.search_and_merge uses vector search, so result may vary slightly if HNSW params differ,
            # but usually deterministic for same query unless index changes.
            # However, running multiple times helps catch any underlying instability or API flakes.
            retrieved_docs_text = pipeline.search_and_merge(query, top_k=5)
            elapsed = time() - start_t

            # B. Scoring (LLM Judge)
            doc_scores = []
            if judge_llm and retrieved_docs_text:
                for doc_text in retrieved_docs_text:
                    try:
                        chain = eval_prompt | judge_llm
                        res_json = chain.invoke(
                            {"question": query, "document": doc_text}
                        )
                        content = (
                            res_json.content.replace("```json", "")
                            .replace("```", "")
                            .strip()
                        )
                        score_dict = json.loads(content)
                        final_score = convert_matches_to_score(score_dict)
                        doc_scores.append(final_score)
                    except Exception as e:
                        # print(f"Scoring Error: {e}")
                        doc_scores.append(0.0)

            mean_score = mean(doc_scores) if doc_scores else 0.0

            results.append(
                {
                    "run_id": run_idx + 1,
                    "query": query,
                    "retrieved_docs": retrieved_docs_text,
                    "doc_scores": doc_scores,
                    "mean_score": mean_score,
                    "latency": elapsed,
                }
            )

        all_runs_results.extend(results)

    # 5. Save Results
    res_df = pd.DataFrame(all_runs_results)
    res_df.to_csv(output_path, index=False)

    avg_score = res_df["mean_score"].mean()
    std_dev = (
        res_df.groupby("run_id")["mean_score"].mean().std() if run_count > 1 else 0.0
    )

    print(f"\nğŸ† Evaluation Complete (x{run_count} Runs)!")
    print(f"   Overall Mean Score: {avg_score:.4f}")
    if run_count > 1:
        print(f"   Standard Deviation: Â± {std_dev:.4f}")
    print(f"   Results saved to: {output_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--csv_path", default="./99_archive/experiments/retrieval.csv")
    parser.add_argument("--output_path", default="./evaluate_hybrid_results_v2.csv")
    parser.add_argument(
        "--limit", type=int, default=None, help="Limit number of queries for test"
    )
    parser.add_argument("--runs", type=int, default=1, help="Number of runs")

    args = parser.parse_args()
    run_local_evaluation(args.csv_path, args.output_path, args.limit, args.runs)
