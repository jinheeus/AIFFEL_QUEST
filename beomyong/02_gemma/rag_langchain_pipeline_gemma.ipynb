{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# LangChain components\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS # Zilliz 대신 FAISS 임포트\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Document Preparation\n",
    "Load the audit cases and prepare them in LangChain's `Document` format. We create two sets of documents for our hybrid search strategy:\n",
    "1.  **Full-text documents:** For semantic search (FAISS).\n",
    "2.  **Keyword-focused documents:** For keyword search (BM25), using only the 'problem' and 'action' fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from audit_cases.json and creating baseline documents from summaries...\n",
      "  - Created 4961 summary-based documents for baseline.\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_docs(filepath=\"audit_cases.json\"):\n",
    "    \"\"\"\n",
    "    [Baseline] 'contents_summary' 필드만을 사용하여 문서를 생성합니다.\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {filepath} and creating baseline documents from summaries...\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        audit_cases = json.load(f)\n",
    "\n",
    "    docs = []\n",
    "    for i, case in enumerate(audit_cases):\n",
    "        site = case.get('site', '알 수 없음')\n",
    "        category = case.get('category', '알 수 없음')\n",
    "        date = case.get('date', '알 수 없음')\n",
    "        original_title = case.get('title', '')\n",
    "        \n",
    "        metadata = {\n",
    "            \"index\": i, \"title\": original_title, \"site\": site,\n",
    "            \"category\": category, \"date\": date\n",
    "        }\n",
    "\n",
    "        summary_dict = {}\n",
    "        summary_str = case.get('contents_summary')\n",
    "        if summary_str:\n",
    "            try:\n",
    "                summary_dict = ast.literal_eval(summary_str)\n",
    "            except (ValueError, SyntaxError):\n",
    "                summary_dict = {}\n",
    "        \n",
    "        title = summary_dict.get('title_str', original_title)\n",
    "        keywords = \", \".join(summary_dict.get('keyword_list', []))\n",
    "        problems = summary_dict.get('problems_str', '')\n",
    "        action = summary_dict.get('action_str', '')\n",
    "        standards = summary_dict.get('standards_str', '')\n",
    "\n",
    "        summary_based_text = (\n",
    "            f\"출처: {site}\\n\"\n",
    "            f\"분류: {category}\\n\"\n",
    "            f\"일자: {date}\\n\"\n",
    "            f\"제목: {title}\\n\"\n",
    "            f\"핵심 키워드: {keywords}\\n\"\n",
    "            f\"문제 요약: {problems}\\n\"\n",
    "            f\"조치 요약: {action}\\n\"\n",
    "            f\"관련 규정: {standards}\"\n",
    "        )\n",
    "        docs.append(Document(page_content=summary_based_text, metadata=metadata))\n",
    "\n",
    "    full_text_documents = docs\n",
    "    keyword_documents = docs\n",
    "\n",
    "    print(f\"  - Created {len(docs)} summary-based documents for baseline.\")\n",
    "    return full_text_documents, keyword_documents\n",
    "\n",
    "full_text_documents, keyword_documents = load_and_prepare_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retriever Setup (Hybrid Search)\n",
    "We'll set up two retrievers and combine them using `EnsembleRetriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models and retrievers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/wm3n_r1s01jfgw6n8j7pr4740000gn/T/ipykernel_10184/599536852.py:3: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=\"http://localhost:11434\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Building FAISS index...\n",
      "    - FAISS retriever ready.\n",
      "  - Building BM25 index...\n",
      "    - BM25 retriever ready.\n",
      "Ensemble retriever ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize models and tokenizers\n",
    "print(\"Initializing models and retrievers...\")\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=\"http://localhost:11434\")\n",
    "okt = Okt()\n",
    "\n",
    "# 1. FAISS (Semantic) Retriever\n",
    "print(\"  - Building FAISS index...\")\n",
    "faiss_vectorstore = FAISS.from_documents(full_text_documents, embeddings)\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(\"    - FAISS retriever ready.\")\n",
    "\n",
    "# 2. BM25 (Keyword) Retriever\n",
    "print(\"  - Building BM25 index...\")\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    documents=keyword_documents, \n",
    "    preprocess_func=lambda s: okt.morphs(s) # Use Okt for tokenization\n",
    ")\n",
    "bm25_retriever.k = 5\n",
    "print(\"    - BM25 retriever ready.\")\n",
    "\n",
    "# 3. Ensemble (Hybrid) Retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[faiss_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5] # Give equal weight to semantic and keyword search\n",
    ")\n",
    "print(\"Ensemble retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Chain Construction (LCEL)\n",
    "Now we define the full RAG chain using LangChain Expression Language (LCEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain constructed successfully with Ollama (gemma3:latest).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/wm3n_r1s01jfgw6n8j7pr4740000gn/T/ipykernel_10184/1146039147.py:16: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"gemma3:latest\", base_url=\"http://localhost:11434\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "당신은 감사 전문가입니다. 제공되는 '관련 감사 사례'를 근거로 하여 사용자의 '질문'에 대해 답변해 주세요. 근거가 부족하면 '정보 없음'으로\n",
    "답하세요.\n",
    "\n",
    "[관련 감사 사례]\n",
    "{context}\n",
    "\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "[답변]\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "# Gemini 대신 Ollama 모델 사용\n",
    "llm = Ollama(model=\"gemma3:latest\", base_url=\"http://localhost:11434\", temperature=0)\n",
    "\n",
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"### 감사사례 (제목: {doc.metadata.get('title', 'N/A')}){doc.page_content}\" for doc in docs])\n",
    "\n",
    "# RAG Chain\n",
    "rag_chain = (\n",
    "    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "                                                                                                                                \n",
    "print(\"RAG chain constructed successfully with Ollama (gemma3:latest).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Queries\n",
    "**이 셀의 `test_query` 변수만 변경하고 이 셀만 반복적으로 실행하여 다양한 질문을 테스트할 수 있습니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running RAG chain for query: '부실시공에 따라 재시공을 하도록 한 감사건도 있나?' ---\n",
      "네, 부실시공에 따라 재시공을 하도록 한 감사건도 있습니다. 다음 감사 사례에서 확인할 수 있습니다.\n",
      "\n",
      "*   **2023년도 부산경남본부 종합감사 결과 (한국도로공사)**: 매입부가가치세 환급업무 부적정 사례에서 “□□본부와 ◰◰지사는 휴게시설부문사업인 '◓◓(ㅍ)주유소 토양오염정밀조사 용역' 등을 매입부가 세계정으로 계리하지 않았습니다. ◰◰지사 등 3개 지사는 가로등 제어기 구매 등의 사업을 시행하면서 도로부문과 휴게시설부문을 구분하여 작성하지 않아 매입부가세계정을 누락시켰습니다. 그 결과로 부가세 15,150,045원을 과다 납부하게 되었습니다.” 이 부분은 부실시공으로 인해 발생하는 추가 비용 발생을 암시합니다.\n",
      "\n",
      "*   **2024년도 특정감사결과_14 (국가철도공단)**: “연약 지반 통로박스 4개소의 신축이음부 이격 현상이 발생하였음. 시공 및 건설 사업 관리 업무의 부적정이 확인됨.” 이 부분은 이격 현상 발생으로 인해 재시공이 필요할 수 있음을 시사합니다.\n",
      "\n",
      "*   **2023년 유지보수 청렴취약분야 운영실태 특정감사 결과 (한국도로공사)**: 차선도색 공사 장비 관리 부적정 사례에서 “부실시공이 발생하였음.” 이 부분은 부실시공으로 인해 재시공이 필요할 수 있음을 시사합니다.\n",
      "\n",
      "이 외에도 다른 감사 사례에서도 부실시공으로 인해 발생하는 문제점을 간접적으로 언급하고 있습니다.--- Execution Complete ---\n"
     ]
    }
   ],
   "source": [
    "test_query = \"부실시공에 따라 재시공을 하도록 한 감사건도 있나?\" # 여기에 질문을 변경하세요!\n",
    "\n",
    "print(f\"--- Running RAG chain for query: '{test_query}' ---\")\n",
    "\n",
    "# Invoke the chain and stream the results\n",
    "for chunk in rag_chain.stream(test_query):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"--- Execution Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
