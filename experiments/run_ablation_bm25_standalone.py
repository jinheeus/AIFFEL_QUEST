import os
import sys
import json
import pandas as pd
from tqdm import tqdm
from statistics import mean
import concurrent.futures

# --- INSTALL COMMANDS (Run these in Colab first) ---
# !pip install -q pymilvus langchain langchain-openai langchain-community sentence-transformers rank_bm25 kiwipiepy tqdm pandas


# --- CONFIGURATION ---
class Config:
    # UPDATE THESE WITH YOUR CREDENTIALS
    MILVUS_URI = os.getenv("MILVUS_URI", "./milvus_demo.db")
    MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
    # Use GPU for Reranker?
    DEVICE = "cuda" if os.getenv("COLAB_GPU") else "cpu"


# --- IMPORTS ---
try:
    from pymilvus import MilvusClient
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from sentence_transformers import CrossEncoder
    from langchain_community.embeddings import HuggingFaceEmbeddings
    from rank_bm25 import BM25Okapi
    from kiwipiepy import Kiwi
    from langchain_core.documents import Document
except ImportError:
    print(
        "âŒ Missing dependencies. Please run:\npip install pymilvus langchain langchain-openai langchain-community sentence-transformers rank_bm25 kiwipiepy tqdm pandas"
    )
    sys.exit(1)


# --- HELPER: Load All Docs for BM25 ---
def load_all_docs(client, collection_name, batch_size=500):
    print(f"ðŸ“¥ Loading corpus from Milvus collection: {collection_name}...")

    # 1. Get total count (approximate or just loop until empty)
    # We'll use offset pagination logic
    all_docs = []
    offset = 0

    while True:
        res = client.query(
            collection_name=collection_name,
            filter="id >= 0",  # Match all
            output_fields=["text", "parent_text", "doc_id"],
            limit=batch_size,
            offset=offset,
        )

        if not res:
            break

        all_docs.extend(res)
        offset += len(res)
        print(f" -> Fetched {len(res)} / Total {len(all_docs)}", end="\r")

        if len(res) < batch_size:
            break

    print(f"\nâœ… Loaded {len(all_docs)} total documents.")
    return all_docs


# --- PIPELINE CLASS (Hybrid) ---
class HybridPipeline:
    def __init__(self):
        print(f"âš™ï¸ Initializing Hybrid Pipeline on {Config.DEVICE}...")

        # 1. Embedding (BGE-M3)
        print("   -> Loading Embedding Model (BGE-M3)...")
        self.embedding = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={"device": Config.DEVICE},
            encode_kwargs={"normalize_embeddings": True},
        )

        # 2. Reranker (BGE-Reranker-v2-m3)
        print("   -> Loading Reranker (BGE-Reranker-v2-m3)...")
        try:
            self.reranker = CrossEncoder(
                "BAAI/bge-reranker-v2-m3",
                max_length=512,
                device=Config.DEVICE,
                automodel_args={"torch_dtype": "auto"},
            )
        except Exception:
            print("   âš ï¸ Failed to load BGE-Reranker, fallback to MiniLM.")
            self.reranker = CrossEncoder(
                "cross-encoder/ms-marco-MiniLM-L-6-v2", device=Config.DEVICE
            )

        # 3. Milvus
        print("   -> Connecting to Milvus...")
        self.client = MilvusClient(uri=Config.MILVUS_URI, token=Config.MILVUS_TOKEN)
        self.collection_name = "audit_rag_hybrid_v1"

        # 4. BM25 Index
        print("   -> Building BM25 Index (This may take a minute)...")
        self.tokenizer = Kiwi()
        raw_docs = load_all_docs(self.client, self.collection_name, batch_size=200)

        self.bm25_corpus = []  # Tokenized Clean Text
        self.bm25_docs = []  # Document Objects (1:1 with corpus)
        self.id_to_doc = {}  # Quick Lookup Hash -> Document

        for d in raw_docs:
            text = d.get("text", "")
            parent = d.get("parent_text", "")

            # Tokenize
            tokens = [t.form for t in self.tokenizer.tokenize(text)]
            self.bm25_corpus.append(tokens)

            # Store
            doc_obj = Document(page_content=text, metadata={"parent_text": parent})
            self.bm25_docs.append(doc_obj)
            self.id_to_doc[text] = doc_obj

        self.bm25 = BM25Okapi(self.bm25_corpus)
        print("âœ… BM25 Ready")

    def search(self, query, top_k=5):
        # 1. Dense Search
        q_vec = self.embedding.embed_query(query)
        dense_res = self.client.search(
            collection_name=self.collection_name,
            data=[q_vec],
            limit=50,
            output_fields=["text", "parent_text"],
        )
        dense_results = []
        for hit in dense_res[0]:
            entity = hit["entity"]
            d = Document(
                page_content=entity.get("text"),
                metadata={"parent_text": entity.get("parent_text")},
            )
            dense_results.append(d)

        # 2. Sparse Search (BM25)
        q_tokens = [t.form for t in self.tokenizer.tokenize(query)]
        sparse_results = self.bm25.get_top_n(q_tokens, self.bm25_docs, n=50)

        # 3. RRF Fusion
        dense_ranks = {doc.page_content: i for i, doc in enumerate(dense_results)}
        sparse_ranks = {doc.page_content: i for i, doc in enumerate(sparse_results)}

        all_content = set(dense_ranks.keys()) | set(sparse_ranks.keys())
        fused_scores = []
        k_rrf = 60

        for content in all_content:
            rank_d = dense_ranks.get(content, float("inf"))
            rank_s = sparse_ranks.get(content, float("inf"))
            score = 0.0
            if rank_d != float("inf"):
                score += 1.0 / (k_rrf + rank_d)
            if rank_s != float("inf"):
                score += 1.0 / (k_rrf + rank_s)
            fused_scores.append((content, score))

        fused_scores.sort(key=lambda x: x[1], reverse=True)
        top_candidates = fused_scores[:50]

        # 4. Prepare for Rerank (Parent Mapping)
        seen_parents = set()
        candidates = []

        for content, _ in top_candidates:
            # We assume content is unique enough to lookup metadata
            if content in self.id_to_doc:
                doc_obj = self.id_to_doc[content]
                parent = doc_obj.metadata.get("parent_text") or content

                h = hash(parent)
                if h not in seen_parents:
                    seen_parents.add(h)
                    candidates.append(parent)

        # 5. Rerank
        if candidates:
            pairs = [
                [query, doc_text] for doc_text in candidates[:30]
            ]  # Limit rerank to top 30 unique parents
            scores_list = self.reranker.predict(pairs)
            scored_candidates = sorted(
                zip(candidates[:30], scores_list), key=lambda x: x[1], reverse=True
            )
            final_docs = [doc for doc, score in scored_candidates[:top_k]]
        else:
            final_docs = []

        return final_docs


# --- EVALUATION LOGIC ---
def convert_matches_to_score(match_dict):
    count = 0
    for val in match_dict.values():
        if isinstance(val, dict):
            if val.get("decision") is True:
                count += 1
        elif val is True:
            count += 1
    if count >= 4:
        return 1.0
    elif count >= 2:
        return 0.6
    elif count == 1:
        return 0.2
    else:
        return 0.0


def evaluate_single(query, pipeline, eval_chain):
    if not query:
        return 0.0, [], "Empty Query"

    try:
        docs = pipeline.search(query)
    except Exception as e:
        return 0.0, [], f"Search Error: {str(e)}"

    scores = []
    reasons = []

    # Evaluate Top 3 for consistency
    for d in docs[:3]:
        try:
            res = eval_chain.invoke({"question": query, "document": d})
            content = res.content.replace("```json", "").replace("```", "").strip()
            # Try to extract JSON if dirty
            start = content.find("{")
            end = content.rfind("}") + 1
            if start != -1 and end != -1:
                content = content[start:end]

            s_dict = json.loads(content)
            scores.append(convert_matches_to_score(s_dict))
            reasons.append(s_dict)
        except Exception as e:
            scores.append(0.0)
            reasons.append(
                {
                    "error": str(e),
                    "raw_content": res.content if "res" in locals() else "No Response",
                }
            )

    final_score = mean(scores) if scores else 0.0
    return final_score, [d[:200] for d in docs], reasons


def main(csv_path="retrieval.csv"):
    pipeline = HybridPipeline()

    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=Config.OPENAI_API_KEY)

    # NOTE: User's updated system prompt
    system_prompt = """
ë‹¹ì‹ ì€ ê°ì‚¬ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸-ë¬¸ì„œ ìœ ì‚¬ë„ í‰ê°€ ì „ë¬¸ê°€ìž…ë‹ˆë‹¤.
ì•„ëž˜ ë‹¤ì„¯ ê°€ì§€ ê¸°ì¤€ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸(question)ê³¼ ë¬¸ì„œ(document)ì˜ ìœ ì‚¬ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ì‹­ì‹œì˜¤.
íŒì •ì€ ì—„ê²©í•˜ê²Œ ìˆ˜í–‰í•˜ë˜, ë³¸ í‰ê°€ëŠ” RAG ì‹œìŠ¤í…œ ê³ ë„í™” ë‹¨ê³„ë³„ ì„±ëŠ¥ ë¹„êµë¥¼ ëª©ì ìœ¼ë¡œ í•˜ë¯€ë¡œ
ê° ê¸°ì¤€ì€ ë…ë¦½ì ìœ¼ë¡œ íŒë‹¨í•˜ë©°, ìƒìœ„ ê¸°ì¤€ì„ ì¶©ì¡±í•˜ì§€ ëª»í•˜ë”ë¼ë„ í•˜ìœ„ ê¸°ì¤€ì„ ê°œë³„ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
TrueëŠ” í•´ë‹¹ ê¸°ì¤€ì„ ëª…í™•ížˆ ì¶©ì¡±í•˜ëŠ” ê²½ìš°ì—ë§Œ ì„ íƒí•˜ì‹­ì‹œì˜¤.

[í‰ê°€ ê¸°ì¤€]

1. ì£¼ì œ ì¼ì¹˜(Topic Match)
   ì§ˆë¬¸ê³¼ ë¬¸ì„œê°€ ë‹¤ë£¨ëŠ” ê°ì‚¬ ë¶„ì•¼ê°€ ì„¸ë¶€ ê°ì‚¬ ë¶„ì•¼ ìˆ˜ì¤€ì—ì„œ ë™ì¼í•  ê²½ìš° topic_matchëŠ” trueìž…ë‹ˆë‹¤.
   ì¶œìž¥ë¹„, ê³„ì•½Â·ìˆ˜ì˜ê³„ì•½, ì¸ì‚¬Â·ë³µë¬´, ê¸ˆí’ˆìˆ˜ìˆ˜ ë“± êµ¬ì²´ ê°ì‚¬ ë¶„ì•¼ê°€ ë™ì¼í•´ì•¼ í•˜ë©°,
   ë‚´ë¶€í†µì œ, ê´€ë¦¬ ë¯¸í¡ê³¼ ê°™ì€ í¬ê´„ì  í‘œí˜„ë§Œ ê³µí†µëœ ê²½ìš°ëŠ” falseìž…ë‹ˆë‹¤.

   ì§ˆë¬¸ì— íŠ¹ì • ì—°ë„, ê¸°ê°„, ì‹œì ì´ ëª…ì‹œëœ ê²½ìš° ë¬¸ì„œì˜ date ê°’ì´ í•´ë‹¹ ê¸°ê°„ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
   ë‚ ì§œ ì¡°ê±´ì´ ë¶ˆì¼ì¹˜í•˜ëŠ” ê²½ìš° topic_matchëŠ” falseìž…ë‹ˆë‹¤.
   ì§ˆë¬¸ì— ë‚ ì§œë‚˜ ê¸°ê°„ ì¡°ê±´ì´ ì—†ëŠ” ê²½ìš°ì—ëŠ” dateë¥¼ íŒë‹¨ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.

2. ì„¸ë¶€ìŸì  ì¼ì¹˜(Sub Issue Match)
   ì§ˆë¬¸ì´ ìš”êµ¬í•˜ëŠ” í•µì‹¬ ìŸì ì´ ë¬¸ì„œì—ì„œ ë‹¤ë£¨ëŠ” êµ¬ì²´ì  ë¬¸ì œì™€ ì§ì ‘ì ìœ¼ë¡œ ëŒ€ì‘ë  ê²½ìš°
   subtopic_matchëŠ” trueìž…ë‹ˆë‹¤.
   ê°ì‚¬ ì‹¤ë¬´ì—ì„œ í†µìš©ë˜ëŠ” ë™ì˜ì–´, í‘œí˜„ ì°¨ì´, ì„œìˆ  ë°©ì‹ ì°¨ì´ëŠ” í—ˆìš©í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
   ë™ì¼í•œ ê°ì‚¬ ë¶„ì•¼ì´ë”ë¼ë„ ë¬¸ì œì˜ ì´ˆì ì´ë‚˜ íŒë‹¨ ëŒ€ìƒì´ ë‹¤ë¥´ë©´ falseìž…ë‹ˆë‹¤.

3. ì‚¬ê±´ ë©”ì»¤ë‹ˆì¦˜ ìœ ì‚¬(Case Mechanism Match)
   ë¬¸ì œ ë°œìƒì˜ ì ˆì°¨ì  íë¦„ì´ë‚˜ ì‚¬ê±´ ì „ê°œ ë°©ì‹ì´ ì§ˆë¬¸ê³¼ ë¬¸ì„œì—ì„œ ë³¸ì§ˆì ìœ¼ë¡œ ìœ ì‚¬í•  ê²½ìš°
   case_structure_matchëŠ” trueìž…ë‹ˆë‹¤.
   ëª¨ë“  ì„¸ë¶€ ë‹¨ê³„ê°€ ì™„ì „ížˆ ë™ì¼í•  í•„ìš”ëŠ” ì—†ìœ¼ë‚˜,
   ì£¼ìš” ì ˆì°¨ ìœ„ë°˜ êµ¬ì¡°ë‚˜ ì‚¬ê±´ ì§„í–‰ ë…¼ë¦¬ê°€ ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ì•¼ í•©ë‹ˆë‹¤.
   ê²°ê³¼ë§Œ ìœ ì‚¬í•˜ê³  ë°œìƒ ê³¼ì •ì˜ êµ¬ì¡°ê°€ ë‹¤ë¥¸ ê²½ìš°ëŠ” falseìž…ë‹ˆë‹¤.

4. ìœ„ë°˜í–‰ìœ„ íŒ¨í„´ ìœ ì‚¬(Violation Pattern Match)
   ìœ„ë°˜ í–‰ìœ„ì˜ ìœ í˜•ì´ ì§ˆë¬¸ê³¼ ë¬¸ì„œì—ì„œ ë™ì¼í•˜ê±°ë‚˜ ê°ì‚¬ ì‹¤ë¬´ìƒ ë™ì¼í•œ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜ë  ìˆ˜ ìžˆëŠ” ê²½ìš°
   violation_pattern_matchëŠ” trueìž…ë‹ˆë‹¤.
   í—ˆìœ„ ì²­êµ¬, ë¶€ë‹¹ ì§€ê¸‰, ê·œì • ë¯¸ì¤€ìˆ˜ ë“± ì‹¤ì§ˆì ìœ¼ë¡œ ë™ì¼í•œ ìœ„ë°˜ íŒ¨í„´ì€ ì¼ì¹˜ë¡œ íŒë‹¨í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
   ìœ„ë°˜ í–‰ìœ„ì˜ ì„±ê²©ì´ ëª…í™•ížˆ ë‹¤ë¥¸ ê²½ìš°ëŠ” falseìž…ë‹ˆë‹¤.

5. ì›ì¸ êµ¬ì¡° ìœ ì‚¬(Cause Pattern Match)
   ë¬¸ì œì˜ ê·¼ë³¸ ì›ì¸ì´ ì§ˆë¬¸ê³¼ ë¬¸ì„œì—ì„œ ë™ì¼í•˜ê±°ë‚˜,
   ë™ì¼í•œ ê´€ë¦¬Â·í†µì œ êµ¬ì¡°ìƒì˜ ì›ì¸ìœ¼ë¡œ ì„¤ëª…ë  ìˆ˜ ìžˆëŠ” ê²½ìš° cause_pattern_matchëŠ” trueìž…ë‹ˆë‹¤.
   ë‚´ë¶€í†µì œ ë¯¸í¡, ê´€ë¦¬Â·ê°ë… ì†Œí™€, ê·œì • ë¯¸ë¹„ ë“± êµ¬ì¡°ì  ì›ì¸ì´ ê³µí†µì ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ë©´ ì¼ì¹˜ë¡œ íŒë‹¨í•  ìˆ˜ ìžˆìŠµë‹ˆë‹¤.
   ê°œì¸ì˜ ê³ ì˜ì  ë¹„ìœ„ë‚˜ ì¼íƒˆ ë“± ì›ì¸ êµ¬ì¡°ê°€ ëª…í™•ížˆ ë‹¤ë¥¸ ê²½ìš°ëŠ” falseìž…ë‹ˆë‹¤.

[ì¶œë ¥ í˜•ì‹]
ë°˜ë“œì‹œ ì•„ëž˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì‹­ì‹œì˜¤.
ê° í•­ëª©ì— ëŒ€í•´ íŒë‹¨ ê²°ê³¼(decision)ì™€ ê·¸ ì´ìœ (reason)ë¥¼ 1~2ë¬¸ìž¥ìœ¼ë¡œ ê°„ëžµížˆ ìž‘ì„±í•˜ì‹­ì‹œì˜¤.
ì´ìœ (reason)ëŠ” ë°˜ë“œì‹œ ì§ˆë¬¸(question)ê³¼ ë¬¸ì„œ(document)ì— ëª…ì‹œì ìœ¼ë¡œ í¬í•¨ëœ ì •ë³´ë§Œì„ ê·¼ê±°ë¡œ ìž‘ì„±í•˜ì‹­ì‹œì˜¤.

{{
  "topic_match": {{
      "decision": true/false,
      "reason": "íŒë‹¨ ê·¼ê±° ìž‘ì„±"
  }},
  "subtopic_match": {{
      "decision": true/false,
      "reason": "íŒë‹¨ ê·¼ê±° ìž‘ì„±"
  }},
  "case_structure_match": {{
      "decision": true/false,
      "reason": "íŒë‹¨ ê·¼ê±° ìž‘ì„±"
  }},
  "violation_pattern_match": {{
      "decision": true/false,
      "reason": "íŒë‹¨ ê·¼ê±° ìž‘ì„±"
  }},
  "cause_pattern_match": {{
      "decision": true/false,
      "reason": "íŒë‹¨ ê·¼ê±° ìž‘ì„±"
  }}
}}

JSON ì´ì™¸ì˜ ì„œë¡ ì´ë‚˜ ì¶”ìž„ìƒˆëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.
"""
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "[Question]\n{question}\n\n[Document]\n{document}"),
        ]
    )
    chain = prompt | llm

    df = pd.read_csv(csv_path)
    print(f"Loaded {len(df)} queries.")

    results = []
    detailed_logs = []

    for i, row in tqdm(df.iterrows(), total=len(df)):
        q = row.get("question") or row.get("query")
        # Pass evaluate_chain explicitly
        score, retrieved_snippets, debug_reasons = evaluate_single(q, pipeline, chain)

        results.append(score)

        detailed_logs.append(
            {
                "question": q,
                "score": score,
                "retrieved_docs_snippets": str(retrieved_snippets),
                "debug_reasons": json.dumps(debug_reasons, ensure_ascii=False),
            }
        )

    print(f"Mean Score: {mean(results):.4f}")

    # Save CSV
    output_path = "ablation_results_bm25_standalone.csv"
    pd.DataFrame(detailed_logs).to_csv(output_path, index=False, encoding="utf-8-sig")
    print(f"âœ… Detailed results saved to {output_path}")


if __name__ == "__main__":
    if len(sys.argv) > 1:
        main(sys.argv[1])
    else:
        print("Usage: python run_ablation_bm25_standalone.py <csv_path>")
