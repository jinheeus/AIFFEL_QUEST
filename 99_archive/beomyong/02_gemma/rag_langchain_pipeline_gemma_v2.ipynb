{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Pipeline with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ast\n",
    "from dotenv import load_dotenv\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "# LangChain components\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Zilliz # FAISS 대신 Zilliz 임포트\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain_classic.retrievers import EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API keys loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "try:\n",
    "    zilliz_uri = os.getenv(\"ZILLIZ_CLOUD_URI\")\n",
    "    zilliz_token = os.getenv(\"ZILLIZ_CLOUD_TOKEN\")\n",
    "\n",
    "    if not zilliz_uri:\n",
    "        raise ValueError(\"ZILLIZ_CLOUD_URI not set in .env file.\")\n",
    "    if not zilliz_token:\n",
    "        raise ValueError(\"ZILLIZ_CLOUD_TOKEN not set in .env file.\")\n",
    "    print(\"API keys loaded successfully.\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Document Preparation\n",
    "Load the audit cases and prepare them in LangChain's `Document` format. We create two sets of documents for our hybrid search strategy:\n",
    "1.  **Summary-based documents:** For semantic search (Zilliz).\n",
    "2.  **Keyword-focused documents:** For keyword search (BM25), using only the 'problem' and 'action' fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ../audit_cases.json and preparing optimized documents...\n",
      "  - Created 4961 documents for semantic search.\n",
      "  - Created 4961 documents for keyword search.\n"
     ]
    }
   ],
   "source": [
    "def load_and_prepare_docs(filepath=\"../audit_cases.json\"):\n",
    "    \"\"\"\n",
    "    의미 검색과 키워드 검색의 역할을 분리하여 문서를 최적화합니다.\n",
    "    1. 의미 검색용: 'contents_summary' 기반의 요약 문서를 사용\n",
    "    2. 키워드 검색용: 'title', 'problem', 'action' 필드를 사용하여 키워드 강화\n",
    "    \"\"\"\n",
    "    print(f\"Loading data from {filepath} and preparing optimized documents...\")\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        audit_cases = json.load(f)\n",
    "\n",
    "    semantic_docs = [] # For FAISS (semantic search)\n",
    "    keyword_docs = []  # For BM25 (keyword search)\n",
    "\n",
    "    for i, case in enumerate(audit_cases):\n",
    "        site = case.get('site', '알 수 없음')\n",
    "        category = case.get('category', '알 수 없음')\n",
    "        date = case.get('date', '알 수 없음')\n",
    "        original_title = case.get('title', '')\n",
    "\n",
    "        metadata = {\n",
    "            \"index\": i, \"title\": original_title, \"site\": site,\n",
    "            \"category\": category, \"date\": date\n",
    "        }\n",
    "\n",
    "        # 1. [의미 검색용 문서] 생성\n",
    "        summary_dict = {}\n",
    "        summary_str = case.get('contents_summary')\n",
    "        if summary_str:\n",
    "            try:\n",
    "                summary_dict = ast.literal_eval(summary_str)\n",
    "            except (ValueError, SyntaxError):\n",
    "                summary_dict = {}\n",
    "\n",
    "        title = summary_dict.get('title_str', original_title)\n",
    "        keywords = \", \".join(summary_dict.get('keyword_list', []))\n",
    "        problems = summary_dict.get('problems_str', '')\n",
    "        action = summary_dict.get('action_str', '')\n",
    "        standards = summary_dict.get('standards_str', '')\n",
    "\n",
    "        summary_based_text = (\n",
    "            f\"출처: {site}\\\\n\"\n",
    "            f\"분류: {category}\\\\n\"\n",
    "            f\"일자: {date}\\\\n\"\n",
    "            f\"제목: {title}\\\\n\"\n",
    "            f\"핵심 키워드: {keywords}\\\\n\"\n",
    "            f\"문제 요약: {problems}\\\\n\"\n",
    "            f\"조치 요약: {action}\\\\n\"\n",
    "            f\"관련 규정: {standards}\"\n",
    "        )\n",
    "        semantic_docs.append(Document(page_content=summary_based_text, metadata=metadata))\n",
    "\n",
    "        # 2. [키워드 검색용 문서] 생성\n",
    "        problem_raw = case.get('problem', '')\n",
    "        action_raw = case.get('action', '')\n",
    "        keyword_optimized_text = f\"제목: {original_title}\\\\n문제점: {problem_raw}\\\\n조치사항: {action_raw}\"\n",
    "        keyword_docs.append(Document(page_content=keyword_optimized_text, metadata=metadata))\n",
    "\n",
    "\n",
    "    print(f\"  - Created {len(semantic_docs)} documents for semantic search.\")\n",
    "    print(f\"  - Created {len(keyword_docs)} documents for keyword search.\")\n",
    "    return semantic_docs, keyword_docs\n",
    "\n",
    "full_text_documents, keyword_documents = load_and_prepare_docs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retriever Setup (Hybrid Search)\n",
    "We'll set up two retrievers and combine them using `EnsembleRetriever`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models and retrievers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/wm3n_r1s01jfgw6n8j7pr4740000gn/T/ipykernel_29820/2468992652.py:3: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=\"http://localhost:11434\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Loading Zilliz vector store...\n",
      "    - Zilliz retriever ready.\n",
      "  - Building BM25 index...\n",
      "    - BM25 retriever ready.\n",
      "Ensemble retriever ready!\n"
     ]
    }
   ],
   "source": [
    "# Initialize models and tokenizers\n",
    "print(\"Initializing models and retrievers...\")\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text\", base_url=\"http://localhost:11434\")\n",
    "okt = Okt()\n",
    "\n",
    "# 1. Zilliz (Semantic) Retriever\n",
    "print(\"  - Loading Zilliz vector store...\")\n",
    "zilliz_vectorstore = Zilliz(\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"audit_cases_gemma_v1\", # Zilliz에 업로드한 컬렉션 이름\n",
    "    connection_args={\"uri\": zilliz_uri, \"token\": zilliz_token}\n",
    ")\n",
    "semantic_retriever = zilliz_vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "print(\"    - Zilliz retriever ready.\")\n",
    "\n",
    "# 2. BM25 (Keyword) Retriever\n",
    "print(\"  - Building BM25 index...\")\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    documents=keyword_documents, \n",
    "    preprocess_func=lambda s: okt.morphs(s) # Use Okt for tokenization\n",
    ")\n",
    "bm25_retriever.k = 5\n",
    "print(\"    - BM25 retriever ready.\")\n",
    "\n",
    "# 3. Ensemble (Hybrid) Retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[semantic_retriever, bm25_retriever],\n",
    "    weights=[0.5, 0.5] # Give equal weight to semantic and keyword search\n",
    ")\n",
    "print(\"Ensemble retriever ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. RAG Chain Construction (LCEL)\n",
    "Now we define the full RAG chain using LangChain Expression Language (LCEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG chain constructed successfully with Ollama (gemma3:latest).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/14/wm3n_r1s01jfgw6n8j7pr4740000gn/T/ipykernel_29820/2933918613.py:16: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the `langchain-ollama package and should be used instead. To use it run `pip install -U `langchain-ollama` and import as `from `langchain_ollama import OllamaLLM``.\n",
      "  llm = Ollama(model=\"gemma3:latest\", base_url=\"http://localhost:11434\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "당신은 감사 전문가입니다. 제공되는 '관련 감사 사례'에 명시적으로 언급된 내용만을 근거로 하여 사용자의 '질문'에 대해 답변해 주세요.\n",
    "주어진 내용에 근거가 부족하면 '정보 없음'으로 답하세요. 절대로 내용을 추론하거나 암시해서는 안 됩니다.\n",
    "\n",
    "[관련 감사 사례]\n",
    "{context}\n",
    "\n",
    "[질문]\n",
    "{question}\n",
    "\n",
    "[답변]\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# LLM 설정 (Ollama - gemma3:latest)\n",
    "llm = Ollama(model=\"gemma3:latest\", base_url=\"http://localhost:11434\", temperature=0)\n",
    "\n",
    "# Helper function to format retrieved documents\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"### 감사사례 (제목: {doc.metadata.get('title', 'N/A')}){doc.page_content}\" for doc in docs])\n",
    "\n",
    "# RAG Chain\n",
    "rag_chain = (\n",
    "    {\"context\": ensemble_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "                                                                                                                                \n",
    "print(\"RAG chain constructed successfully with Ollama (gemma3:latest).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Queries\n",
    "**이 셀의 `test_query` 변수만 변경하고 이 셀만 반복적으로 실행하여 다양한 질문을 테스트할 수 있습니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Running RAG chain for query: '부실시공에 따라 재시공을 하도록 한 감사건도 있나?' ---\n",
      "네, 부실시공에 따라 재시공을 하도록 한 감사건이 있습니다.\n",
      "\n",
      "감사원 감기간 중 실시한 전문가 자문 결과, 최대 균열 폭 1.4 ㎜의 균열이 지하차도 박스 및 옹벽 구간 전반에 걸쳐 발생했으며, 박스 구간의 포장 불량률이 재포장 기준 (10%) 을 초과 (14.3%, 옹벽 구간 8.4%) 한 것으로 나타났습니다. 이로 인해 재포장이 필요하게 되었습니다.\n",
      "\n",
      "--- Execution Complete ---\n"
     ]
    }
   ],
   "source": [
    "test_query = \"부실시공에 따라 재시공을 하도록 한 감사건도 있나?\" # 여기에 질문을 변경하세요!\n",
    "\n",
    "print(f\"--- Running RAG chain for query: '{test_query}' ---\")\n",
    "\n",
    "# Invoke the chain and stream the results\n",
    "for chunk in rag_chain.stream(test_query):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n--- Execution Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
