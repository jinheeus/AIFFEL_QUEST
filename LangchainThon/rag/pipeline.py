# rag/pipeline.py

import json
from typing import List, Dict, Any

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

from schemas import JDStructured, FullReport
from prompts import SYSTEM_PROMPT, JD_STRUCTURE_PROMPT, REPORT_PROMPT
from rag.retriever import build_retriever
from config import settings

from .retriever import retrieve_evidence


def _llm(streaming: bool = False):
    return ChatOpenAI(
        model=settings.openai_model,
        temperature=settings.temperature,
        streaming=streaming,
    )

def structure_jd(jd_text: str) -> JDStructured:
    """
    JD ì›ë¬¸ì„ JDStructured ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜.
    LLM ì¶œë ¥ì´ ìŠ¤í‚¤ë§ˆì™€ ì‚´ì§ ì–´ê¸‹ë‚˜ë”ë¼ë„ ì •ê·œí™”í•˜ì—¬ ì•ˆì „í•˜ê²Œ ë°˜í™˜.
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", SYSTEM_PROMPT),
            ("user", JD_STRUCTURE_PROMPT),
        ]
    )

    llm = _llm(streaming=False).with_structured_output(
        JDStructured,
        method="function_calling",
    )

    chain = prompt | llm

    raw = chain.invoke({"jd_text": jd_text})

      # -------------------------
    # ğŸ”¥ ì•ˆì „ì„± ë³´ì • ë‹¨ê³„ (ì—¬ê¸° ê°•í™”ë¨)
    # -------------------------
    def _fix_list_str(x):
        if x is None:
            return []

        fixed = []
        for v in x:
            # ì´ë¯¸ ë¬¸ìì—´ì´ë©´ ê·¸ëŒ€ë¡œ
            if isinstance(v, str):
                fixed.append(v)
            # {"ë¶ˆëª…í™•":"ë‚´ìš©"} -> ê°’ë§Œ êº¼ë‚´ê¸°
            elif isinstance(v, dict):
                # dictì˜ valueë¥¼ ì „ë¶€ ì—°ê²°
                fixed.append(" ".join(str(val) for val in v.values()))
            # ë¦¬ìŠ¤íŠ¸ê°€ ë˜ ë“¤ì–´ì˜¨ ê²½ìš°
            elif isinstance(v, list):
                fixed.append(" ".join(str(val) for val in v))
            # ê·¸ ì™¸ ìˆ«ì, bool ë‹¤ ë¬¸ìì—´í™”
            else:
                fixed.append(str(v))

        return fixed

    # ğŸ”½ JDStructured ë‚´ë¶€ í•„ë“œ ê°’ ì•ˆì „í•˜ê²Œ êµì²´
    raw.core_competencies = _fix_list_str(getattr(raw, "core_competencies", []))
    raw.requirements = _fix_list_str(getattr(raw, "requirements", []))
    raw.preferred = _fix_list_str(getattr(raw, "preferred", []))
    raw.responsibilities = _fix_list_str(getattr(raw, "responsibilities", []))
    raw.tech_stack = _fix_list_str(getattr(raw, "tech_stack", []))

    return raw


def generate_report(
    jd_struct: JDStructured, essays: List[Dict[str, Any]], retrieved_summaries: List[str]
) -> FullReport:
    """
    JD êµ¬ì¡° + ìì†Œì„œ + ìœ ì‚¬ì‚¬ë¡€ ìš”ì•½ì„ ë„£ê³  FullReport ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë¦¬í¬íŠ¸ ìƒì„±.
    """
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", SYSTEM_PROMPT),
            ("user", REPORT_PROMPT),
        ]
    )

    # FullReport ìŠ¤í‚¤ë§ˆë¡œ function calling ê¸°ë°˜ structured output
    llm = _llm(streaming=False).with_structured_output(
        FullReport,
        method="function_calling",
    )

    chain = prompt | llm

    return chain.invoke(
        {
            "jd_json": jd_struct.model_dump_json(ensure_ascii=False),
            "essays_json": json.dumps(essays, ensure_ascii=False),
            "retrieved_summaries": json.dumps(
                retrieved_summaries,
                ensure_ascii=False,
            ),
        }
    )


def build_all(
    jd_text: str,
    essays: List[Dict[str, Any]],
    user_job: str = "",
    user_stack: str = "",
) -> FullReport:
    """
    í•œ ë²ˆ í˜¸ì¶œë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰:
    JD êµ¬ì¡°í™” â†’ ìœ ì‚¬ ì‚¬ë¡€ ê²€ìƒ‰ â†’ ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
    """
    jd_struct = structure_jd(jd_text)
    evidence = retrieve_evidence(jd_struct, user_job=user_job, user_stack=user_stack)
    report = generate_report(
        jd_struct,
        essays=essays,
        retrieved_summaries=evidence,
    )
    return report
