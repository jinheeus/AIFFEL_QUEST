import os
import json
import time
import ast
from dotenv import load_dotenv
from langchain_core.documents import Document
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Zilliz

def load_env_variables():
    """
    .env íŒŒì¼ì—ì„œ Zilliz ì—°ê²° ì •ë³´ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
    """
    load_dotenv()
    zilliz_uri = os.getenv("ZILLIZ_CLOUD_URI")
    zilliz_token = os.getenv("ZILLIZ_CLOUD_TOKEN")

    if not zilliz_uri:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ 'ZILLIZ_CLOUD_URI'ê°€ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    if not zilliz_token:
        raise ValueError("í™˜ê²½ ë³€ìˆ˜ 'ZILLIZ_CLOUD_TOKEN'ì´ .env íŒŒì¼ì— ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
    print("âœ… Zilliz ì—°ê²° ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return zilliz_uri, zilliz_token

def load_and_prepare_docs(filepath="audit_cases.json"):
    """
    [Baseline] 'contents_summary' í•„ë“œë§Œì„ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ“„ '{filepath}' íŒŒì¼ì—ì„œ 'contents_summary' ê¸°ë°˜ ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    with open(filepath, 'r', encoding='utf-8') as f:
        audit_cases = json.load(f)

    docs = []
    for i, case in enumerate(audit_cases):
        site = case.get('site', 'ì•Œ ìˆ˜ ì—†ìŒ')
        category = case.get('category', 'ì•Œ ìˆ˜ ì—†ìŒ')
        date = case.get('date', 'ì•Œ ìˆ˜ ì—†ìŒ')
        original_title = case.get('title', '')
        
        metadata = {"index": i, "title": original_title, "site": site, "category": category, "date": date}

        summary_dict = {}
        summary_str = case.get('contents_summary')
        if summary_str:
            try:
                summary_dict = ast.literal_eval(summary_str)
            except (ValueError, SyntaxError):
                summary_dict = {}
        
        title = summary_dict.get('title_str', original_title)
        keywords = ", ".join(summary_dict.get('keyword_list', []))
        problems = summary_dict.get('problems_str', '')
        action = summary_dict.get('action_str', '')
        standards = summary_dict.get('standards_str', '')

        summary_based_text = (
            f"ì¶œì²˜: {site}\n"
            f"ë¶„ë¥˜: {category}\n"
            f"ì¼ì: {date}\n"
            f"ì œëª©: {title}\n"
            f"í•µì‹¬ í‚¤ì›Œë“œ: {keywords}\n"
            f"ë¬¸ì œ ìš”ì•½: {problems}\n"
            f"ì¡°ì¹˜ ìš”ì•½: {action}\n"
            f"ê´€ë ¨ ê·œì •: {standards}"
        )
        docs.append(Document(page_content=summary_based_text, metadata=metadata))

    print(f"  - ì´ {len(docs)}ê°œì˜ ìš”ì•½ ê¸°ë°˜ ë¬¸ì„œë¥¼ ì¤€ë¹„í–ˆìŠµë‹ˆë‹¤.")
    return docs

def main():
    """
    ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  Ollamaë¡œ ì„ë² ë”©í•˜ì—¬ Zilliz Cloudì— ì—…ë¡œë“œí•˜ëŠ” ë©”ì¸ í•¨ìˆ˜
    """
    COLLECTION_NAME = "audit_cases_gemma_v1"
    try:
        zilliz_uri, zilliz_token = load_env_variables()
    except ValueError as e:
        print(f"ğŸš¨ ì—ëŸ¬: {e}")
        return

    # 1. ë°ì´í„° ë¡œë“œ
    documents = load_and_prepare_docs()
    if not documents:
        return

    # 2. ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (Ollama)
    print("\nğŸ§  Ollama ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤ (nomic-embed-text)...")
    try:
        embeddings = OllamaEmbeddings(model="nomic-embed-text", base_url="http://localhost:11434")
    except Exception as e:
        print(f"ğŸš¨ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        print("   'ollama'ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€, 'nomic-embed-text' ëª¨ë¸ì´ pull ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        return

    # 3. Zillizì— ë°ì´í„° ì—…ë¡œë“œ (ë°°ì¹˜ ì²˜ë¦¬)
    print(f"\nâ˜ï¸ Zilliz Cloudì— ë°ì´í„°ë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤ (Collection: '{COLLECTION_NAME}')...")
    
    batch_size = 128
    total_batches = (len(documents) -1) // batch_size + 1

    try:
        # ì²« ë²ˆì§¸ ë°°ì¹˜ë¡œ ë²¡í„° ì €ì¥ì†Œ ìƒì„±
        print(f"  - [1/{total_batches} ë°°ì¹˜] ì²˜ë¦¬ ì¤‘...")
        vector_store = Zilliz.from_documents(
            documents=documents[:batch_size],
            embedding=embeddings,
            collection_name=COLLECTION_NAME,
            connection_args={"uri": zilliz_uri, "token": zilliz_token},
            auto_id=True,
            drop_old=True
        )
        print("  - ì²« ë²ˆì§¸ ë°°ì¹˜ ì™„ë£Œ.")
        time.sleep(1)

        # ë‚˜ë¨¸ì§€ ë°°ì¹˜ ì¶”ê°€
        for i in range(batch_size, len(documents), batch_size):
            batch_num = (i // batch_size) + 1
            batch_docs = documents[i:i+batch_size]
            
            print(f"  - [{batch_num}/{total_batches} ë°°ì¹˜] ì²˜ë¦¬ ì¤‘...")
            vector_store.add_documents(batch_docs)
            print(f"  - {batch_num}ë²ˆì§¸ ë°°ì¹˜ ì™„ë£Œ.")
            time.sleep(1)
        
        print("\nâœ¨ ëª¨ë“  ë¬¸ì„œì˜ ì„ë² ë”© ë° Zilliz Cloud ì—…ë¡œë“œë¥¼ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤!")

    except Exception as e:
        print(f"\nğŸš¨ Zilliz ì—…ë¡œë“œ ì¤‘ ì—ëŸ¬ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        print("   - Zilliz Cloud URIì™€ Tokenì´ ì˜¬ë°”ë¥¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print("   - 'pymilvus', 'langchain-community' íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")

if __name__ == "__main__":
    main()
