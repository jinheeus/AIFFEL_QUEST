import os
import sys
import json
import pandas as pd
from dotenv import load_dotenv

# Load env vars
load_dotenv()

# Import the pipeline class from the standalone script (assuming it's in the same dir)
# We need to adapt this because we can't easily import from the standalone script if it's not a module.
# proper way: I will copy the minimal logic here to ensure I'm testing the SAME code structure.

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pymilvus import MilvusClient
from langchain_community.embeddings import HuggingFaceEmbeddings
from sentence_transformers import CrossEncoder
from statistics import mean


class Config:
    MILVUS_URI = os.getenv("MILVUS_URI", "./milvus_demo.db")
    MILVUS_TOKEN = os.getenv("MILVUS_TOKEN", "")
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
    DEVICE = "cpu"  # Force CPU for local debug to avoid CUDA errors if not available


def debug_run():
    print("ğŸš€ Starting Local Debug...")

    # 1. Test OpenAI Connection
    if not Config.OPENAI_API_KEY:
        print("âŒ OpenAI API Key is MISSING in environment!")
        return
    print("âœ… OpenAI API Key found.")

    # 2. Test Milvus Connection
    print(f"ğŸ”Œ Connecting to Milvus: {Config.MILVUS_URI}")
    try:
        client = MilvusClient(uri=Config.MILVUS_URI, token=Config.MILVUS_TOKEN)
        # Just check connection by describing collection or simple query
        res = client.query(
            collection_name="audit_rag_hybrid_v1",
            filter="id >= 0",
            output_fields=["doc_id"],
            limit=1,
        )
        print("âœ… Milvus Connected. Sample Query Success.")
    except Exception as e:
        print(f"âŒ Milvus Connection Failed: {e}")
        return

    # 3. Initialize Pipeline Components (Mocking the StandalonePipeline structure)
    print("âš™ï¸ Loading Embedding Model (BGE-M3)...")
    try:
        embedding = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={"device": Config.DEVICE},
            encode_kwargs={"normalize_embeddings": True},
        )
        print("âœ… Embedding Model Loaded.")
    except Exception as e:
        print(f"âŒ Embedding Model Failed: {e}")
        return

    # 4. Perform Search
    query = "ì¶œì¥ë¹„ ë¶€ì • ìˆ˜ë ¹ ì‚¬ë¡€"
    print(f"ğŸ” Searching for: '{query}'")

    try:
        q_vec = embedding.embed_query(query)
        res = client.search(
            collection_name="audit_rag_hybrid_v1",
            data=[q_vec],
            limit=3,
            output_fields=["text", "parent_text", "doc_id"],
        )
        docs = [h["entity"]["text"] for h in res[0]]
        print(f"âœ… Retrieved {len(docs)} documents.")
        if not docs:
            print("âš ï¸ No documents found! This explains the 0.0 score.")
            return
        print(f"   Sample Doc: {docs[0][:100]}...")
    except Exception as e:
        print(f"âŒ Search Failed: {e}")
        return

    # 5. Test LLM Evaluation (The likely culprit for 0.0 if search works)
    print("âš–ï¸ Testing LLM Evaluation...")
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key=Config.OPENAI_API_KEY)

    system_prompt = """
    ë‹¹ì‹ ì€ ê°ì‚¬ë¬¸ì„œ ê¸°ë°˜ ì§ˆë¬¸-ë¬¸ì„œ ìœ ì‚¬ë„ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ... (Simplified for Debug) ...
    [ì¶œë ¥ í˜•ì‹]
    ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥: {{"topic_match": {{"decision": true, "reason": "..."}}}}
    """

    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", system_prompt),
            ("human", "[Question]\n{question}\n\n[Document]\n{document}"),
        ]
    )
    chain = prompt | llm

    try:
        print("   Invoking LLM...")
        res = chain.invoke({"question": query, "document": docs[0]})
        print(f"   LLM Raw Output:\n{res.content}")

        # Parse Check
        content = res.content.replace("```json", "").replace("```", "").strip()
        start = content.find("{")
        end = content.rfind("}") + 1
        if start != -1 and end != -1:
            content = content[start:end]

        data = json.loads(content)
        print("âœ… JSON Parsing Success:", data.keys())

    except Exception as e:
        print(f"âŒ LLM Evaluation Failed: {e}")


if __name__ == "__main__":
    debug_run()
